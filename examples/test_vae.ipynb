{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb0318d1-f1d8-43e0-bde8-127ceb6323b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/fs/ragr-research/users/yihangs/miniconda3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from vae import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4357334-a8f2-490b-ade5-c992b313ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_clustered_nb(\n",
    "    n_clusters=4,\n",
    "    points_per_cluster=300,\n",
    "    n_features=60,\n",
    "    latent_dim=2,          # keep at 2 for direct plotting\n",
    "    theta_val=12.0,\n",
    "    radius=3.0,\n",
    "    cluster_spread=0.25,   # smaller => tighter clusters\n",
    "    seed=123,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a clean clusterable dataset:\n",
    "      z | c ~ N(center[c], cluster_spread^2 I) in R^2\n",
    "      logits = z @ W^T + b\n",
    "      px_scale = softmax(logits)\n",
    "      mu = library * px_scale, library ~ LogNormal\n",
    "      x ~ NB(mean=mu, total_count=theta_val)\n",
    "    Returns: counts X [N,D], labels [N], true Z [N,2]\n",
    "    \"\"\"\n",
    "    g = torch.Generator(device=device).manual_seed(seed)\n",
    "    N = n_clusters * points_per_cluster\n",
    "\n",
    "    # Arrange cluster centers on a circle\n",
    "    centers = []\n",
    "    for k in range(n_clusters):\n",
    "        angle = 2 * math.pi * k / n_clusters\n",
    "        centers.append([radius * math.cos(angle), radius * math.sin(angle)])\n",
    "    centers = torch.tensor(centers, dtype=torch.float32, device=device)  # [K,2]\n",
    "\n",
    "    # Sample cluster assignments\n",
    "    labels = torch.arange(n_clusters, device=device).repeat_interleave(points_per_cluster)  # [N]\n",
    "    # True latent z\n",
    "    z = torch.randn(N, latent_dim, generator=g, device=device) * cluster_spread\n",
    "    z += centers[labels]  # shift by cluster center -> tight, separated clusters\n",
    "\n",
    "    # Decoder-ish linear map to gene space (fixed for data gen)\n",
    "    W = torch.randn(n_features, latent_dim, generator=g, device=device) / math.sqrt(latent_dim)\n",
    "    b = torch.randn(n_features, generator=g, device=device) * 0.2\n",
    "\n",
    "    logits = z @ W.T + b                              # [N,D]\n",
    "    px_scale = F.softmax(logits, dim=-1)              # proportions\n",
    "    # Log-normal library sizes around ~5k counts\n",
    "    lib = torch.exp(torch.randn(N, 1, generator=g, device=device) * 0.25 + 8.5)\n",
    "    mu = lib * px_scale\n",
    "\n",
    "    theta = torch.full_like(mu, float(theta_val))\n",
    "    p = theta / (theta + mu + 1e-8)\n",
    "    nb = NegativeBinomial(total_count=theta, probs=p)\n",
    "    X = nb.sample()                                   # integer counts\n",
    "\n",
    "    return X, labels.cpu(), z.cpu()\n",
    "\n",
    "def plot_latents(true_z, enc_mu, labels, title_suffix=\"\"):\n",
    "    labels = labels.numpy()\n",
    "    fig, axes = plt.subplots(1, 2 + int(USE_UMAP), figsize=(12 if USE_UMAP else 8, 4))\n",
    "\n",
    "    # True latent\n",
    "    ax = axes[0]\n",
    "    sc = ax.scatter(true_z[:,0], true_z[:,1], c=labels, s=8, cmap=\"tab10\")\n",
    "    ax.set_title(f\"Ground-truth latent{title_suffix}\")\n",
    "    ax.set_xlabel(\"z1\"); ax.set_ylabel(\"z2\")\n",
    "\n",
    "    # Encoder mean\n",
    "    ax = axes[1]\n",
    "    sc = ax.scatter(enc_mu[:,0], enc_mu[:,1], c=labels, s=8, cmap=\"tab10\")\n",
    "    ax.set_title(f\"Encoder μ (n_latent=2){title_suffix}\")\n",
    "    ax.set_xlabel(\"μ1\"); ax.set_ylabel(\"μ2\")\n",
    "\n",
    "    if USE_UMAP:\n",
    "        reducer = umap.UMAP(n_neighbors=20, min_dist=0.1, random_state=42)\n",
    "        U = reducer.fit_transform(enc_mu)\n",
    "        ax = axes[2]\n",
    "        sc = ax.scatter(U[:,0], U[:,1], c=labels, s=8, cmap=\"tab10\")\n",
    "        ax.set_title(f\"UMAP of encoder μ{title_suffix}\")\n",
    "        ax.set_xlabel(\"UMAP-1\"); ax.set_ylabel(\"UMAP-2\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc9ee87-0fe6-4a7f-a75f-0735c564054d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001] loss=8213.195  kl_w=0.12\n",
      "[005] loss=1547.466  kl_w=0.62\n",
      "[010] loss=1041.444  kl_w=1.00\n",
      "[015] loss=889.046  kl_w=1.00\n",
      "[020] loss=560.827  kl_w=1.00\n",
      "[025] loss=402.375  kl_w=1.00\n",
      "[030] loss=337.642  kl_w=1.00\n",
      "[035] loss=294.564  kl_w=1.00\n",
      "[040] loss=271.401  kl_w=1.00\n",
      "[045] loss=252.735  kl_w=1.00\n",
      "[050] loss=254.150  kl_w=1.00\n",
      "[055] loss=244.187  kl_w=1.00\n",
      "[060] loss=238.008  kl_w=1.00\n",
      "[065] loss=231.625  kl_w=1.00\n",
      "[070] loss=236.039  kl_w=1.00\n",
      "[075] loss=231.494  kl_w=1.00\n",
      "[080] loss=227.650  kl_w=1.00\n",
      "[085] loss=232.370  kl_w=1.00\n",
      "[090] loss=230.033  kl_w=1.00\n",
      "[095] loss=227.422  kl_w=1.00\n",
      "[100] loss=223.145  kl_w=1.00\n",
      "[105] loss=223.382  kl_w=1.00\n",
      "[110] loss=223.402  kl_w=1.00\n",
      "[115] loss=227.588  kl_w=1.00\n",
      "[120] loss=221.433  kl_w=1.00\n",
      "[125] loss=220.605  kl_w=1.00\n",
      "[130] loss=220.422  kl_w=1.00\n"
     ]
    }
   ],
   "source": [
    "USE_UMAP = True\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---- generate clearly separated clusters ----\n",
    "X, y, Z_true = generate_clustered_nb(\n",
    "    n_clusters=5, points_per_cluster=250, n_features=60,\n",
    "    latent_dim=2, theta_val=12.0, radius=3.0, cluster_spread=0.20,\n",
    "    seed=7, device=device\n",
    ")\n",
    "\n",
    "# ---- build and train VAE (n_latent=2) ----\n",
    "model = VAE(n_input=X.shape[1], n_latent=2, n_hidden=128, n_layers=2).to(device)\n",
    "\n",
    "dl = DataLoader(TensorDataset(X.float()), batch_size=128, shuffle=True)\n",
    "# Gentle optimizer is important for stability\n",
    "opt = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "\n",
    "def fit_step(model, xb, kl_w):\n",
    "    fwd = model(xb)\n",
    "    losses = model.loss(xb, fwd, kl_weight=kl_w)\n",
    "    return losses\n",
    "\n",
    "model.train()\n",
    "epochs, kl_warm = 200, 8\n",
    "for ep in range(1, epochs+1):\n",
    "    kl_w = min(1.0, ep / kl_warm)\n",
    "    tot = 0.0; n = 0\n",
    "    for (xb,) in dl:\n",
    "        xb = xb.to(device).float()\n",
    "        opt.zero_grad()\n",
    "        losses = fit_step(model, xb, kl_w)\n",
    "        losses[\"loss\"].backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        opt.step()\n",
    "        tot += losses[\"loss\"].item() * xb.size(0)\n",
    "        n += xb.size(0)\n",
    "    if ep == 1 or ep % 5 == 0 or ep == epochs:\n",
    "        print(f\"[{ep:03d}] loss={tot/n:.3f}  kl_w={kl_w:.2f}\")\n",
    "\n",
    "# ---- encode the full dataset and visualize ----\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    mu, _ = model._get_latent_params(X.float().to(device))\n",
    "mu = mu.cpu().numpy()\n",
    "plot_latents(Z_true.numpy(), mu, y, title_suffix=\" (clustered NB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedbca95-84e3-4ae2-aeab-c8955e2ece96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
